# -*- coding: utf-8 -*-
"""Classificação de Credit Score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O1CzGl1BRnKz-0LMtb1ok9RcXz1sKH-s

**RM348231 - Bianca Firmino Ferreira de Sena - 3DTSR**

# **1-	Criar um modelo de classificação e aplicar na base e colher e explicar os resultados.**

### **Bibliotecas**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

"""Com objetivo de importar varias bibliotecas com modulos de dados.

### **Listas de Modelos**
"""

models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree Classifier': DecisionTreeClassifier(),
    'Naive Bayes': GaussianNB(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(),
    'Random Forest Classifier': RandomForestClassifier()
}

"""O objetivo desse código é criar um dicionário chamado models que associa nomes de algoritmos de aprendizado.

### **Carregar os dados**
"""

data = pd.read_csv("train.csv")
print(data.head(10))

"""Possui o objetivo de carregar os dados do arquivo CSV "train.csv", usando a biblioteca Pandas.

### **Colunas**
"""

print(data.columns)

"""Exibir os nomes das colunas apresentadas na base.

### **Separar as colunas categóricas e numéricas**
"""

categorical_columns = ['Month', 'Name', 'SSN', 'Occupation', 'Type_pf_Loan', 'Payment_Behaviour']  # Lista das colunas categóricas
numeric_columns = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age', 'Payment_of_min_Amount', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Monthly_Balance', 'Credit_Score']  # Lista das colunas numéricas

"""Objetivo de criar lista separada e possui cateorias distintas: Colunas Categóricas e Colunas Númericas.

### **Remover a coluna 'ID' dos dados**
"""

data.drop("ID", axis=1, inplace=True)

"""Remover a coluna chamada "ID" do DataFrame data.

### **Linhas removidas com valores ausentes**
"""

data.dropna(inplace=True)

"""Possui o objetivo de remover todas as linhas do DataFrame data que contenham pelo menos um valor ausente (NaN).

### **Separar features e target**
"""

X = data.drop("Credit_Score", axis=1)
y = data["Credit_Score"]

"""Objetivo de separar o conjunto de dados em duas estruturas: Entrada e Saida, para treinar e avaliar modelos de previsão.

### **Garantir que todas as colunas sejam numéricas**
"""

X = X.apply(pd.to_numeric, errors='coerce')

"""Converter todas as colunas em númericas.

### **Converter colunas categóricas usando one-hot encoding**
"""

X = pd.get_dummies(X, columns=X.select_dtypes(include=['object']).columns)

"""### **Dividir em conjuntos de treinamento e teste**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Realizar a codificação one-hot para as colunas do DataFrame X que contêm tipos de dados "object" como colunas categóricas.

### **Imputação para preencher valores ausentes (NaN)**
"""

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

"""Tratamento de valores ausentes nas variáveis do conjunto de dados, preenchendo-os com a média dos valores não ausentes.

### **Pré-processamento: Padronizar as features numéricas e codificar as categóricas**
"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Possui o objetivo de padronização dos dados nas colunas de recursos, para execução de conjunto no treinamento e teste.

### **Imputação de Valores Ausentes**
"""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')

X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

"""Objetivo de realizar a imputação de valores ausentes no treinamento(X_train) e teste (X_test) utilizando a estratégia da média (mean) para preencher os valores ausentes.

### **Modelo de Regressão Logística e aplicar na base**
"""

import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

accuracies = []


hyperparameter_values = range(1, 21)

for depth in hyperparameter_values:
    dt_classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)
    dt_classifier.fit(X_train, y_train)
    dt_predictions = dt_classifier.predict(X_test)
    dt_accuracy = accuracy_score(y_test, dt_predictions)

    accuracies.append(dt_accuracy)

print("Decision Tree Classifier Accuracy:", dt_accuracy)
print(classification_report(y_test, dt_predictions))

# Gráfico de evolução da precisão
plt.figure(figsize=(10, 6))
plt.plot(hyperparameter_values, accuracies, marker='o', linestyle='-')
plt.xlabel('Max Depth of Decision Tree')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Max Depth of Decision Tree')
plt.grid(True)
plt.show()

"""Possui o objetivo de avaliar como a precisão (acurácia) de um classificador de Árvore de Decisão (Decision Tree Classifier) se comporta à medida que um hiperparâmetro específico, a profundidade máxima da árvore, é ajustado em um intervalo específico. Esse procedimento visa entender como o desempenho do modelo varia em relação à complexidade da árvore de decisão, permitindo identificar a profundidade ideal da árvore para o problema em questão.

### **Regressão Logistica**
"""

import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

accuracies = []

C_values = [0.001, 0.01, 0.1, 1, 10, 100]

for C in C_values:
    lr_classifier = LogisticRegression(C=C, random_state=42)
    lr_classifier.fit(X_train, y_train)
    lr_predictions = lr_classifier.predict(X_test)
    lr_accuracy = accuracy_score(y_test, lr_predictions)

    accuracies.append(lr_accuracy)

print("Logistic Regression Accuracy:", lr_accuracy)
print(classification_report(y_test, lr_predictions))

# Gráfico de evolução da precisão
plt.figure(figsize=(10, 6))
plt.semilogx(C_values, accuracies, marker='o', linestyle='-')
plt.xlabel('C (Regularization Parameter)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Regularization Parameter (Logistic Regression)')
plt.grid(True)
plt.show()

"""Possui o objetivo de analisar como a precisão de um classificador de **Regressão Logística** varia com diferentes níveis de regularização, representados pelo hiperparâmetro C. Isso ajuda a entender como a complexidade do modelo é afetada pela regularização.

### **Naive Bayes**
"""

import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

accuracies = []

train_sizes = [0.6, 0.7, 0.8, 0.9]

for train_size in train_sizes:
    X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, train_size=train_size, random_state=42)

    nb_classifier = GaussianNB()
    nb_classifier.fit(X_train_subset, y_train_subset)
    nb_predictions = nb_classifier.predict(X_test)

    nb_accuracy = accuracy_score(y_test, nb_predictions)
    accuracies.append(nb_accuracy)
    nb_classifier = GaussianNB()
    nb_classifier.fit(X_train, y_train)
    nb_predictions = nb_classifier.predict(X_test)
    nb_accuracy = accuracy_score(y_test, nb_predictions)

    print("Naive Bayes Accuracy:", nb_accuracy)
    print(classification_report(y_test, nb_predictions))

# Gráfico de evolução da precisão

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, marker='o', linestyle='-')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Training Set Size (Naive Bayes)')
plt.grid(True)
plt.show()

"""Possui o objetivo de analisar a precisão de um classificador **Naive Bayes Gaussiano** (GaussianNB) é influenciada pelo tamanho do conjunto de treinamento. Isso ajuda a compreender como a capacidade de generalização do modelo varia com diferentes tamanhos de conjuntos de treinamento.

### **K-Nearest Neighbors (KNN)**
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

accuracies = []
k_values = range(1, 21)

for k in k_values:
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(X_train, y_train)

    knn_predictions = knn_classifier.predict(X_test)
    knn_accuracy = accuracy_score(y_test, knn_predictions)
    accuracies.append(knn_accuracy)

    print("K-Nearest Neighbors Accuracy:", knn_accuracy)
    print(classification_report(y_test, knn_predictions))

# Gráfico de evolução da precisão

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, marker='o', linestyle='-')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Number of Neighbors (KNN)')
plt.grid(True)
plt.show()

"""Com o objetivo de analisar a precisão de um classificador K-Vizinhos mais Próximos (KNN) é influenciada pela variação do número de vizinhos (hiperparâmetro k) em um intervalo específico. Isso ajuda a determinar qual valor de k proporciona o melhor desempenho do modelo KNN.

### **Support Vector Machine (SVM)**
"""

import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

accuracies = []

C_values = [0.001, 0.01, 0.1, 1, 10, 100]

for C in C_values:
    svm_classifier = SVC(C=C, random_state=42)
    svm_classifier.fit(X_train, y_train)
    svm_predictions = svm_classifier.predict(X_test)
    svm_accuracy = accuracy_score(y_test, svm_predictions)

    accuracies.append(svm_accuracy)

# Gráfico de evolução da precisão
plt.figure(figsize=(10, 6))
plt.semilogx(C_values, accuracies, marker='o', linestyle='-')
plt.xlabel('C (Regularization Parameter)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Regularization Parameter (SVM)')
plt.grid(True)
plt.show()

"""Analisar a precisão de um classificador **Support Vector Machine (SVM)** onde é influenciada pela variação do hiperparâmetro de regularização C no conjunto específico de valores. O objetivo é entender como a escolha do parâmetro de regularização C afeta a precisão do modelo SVM e identificar o valor de C que resulta no melhor desempenho.

### **Regressão Logistica X Naive Bayes**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

np.random.seed(42)
X = np.random.randn(100, 2)
y = np.random.randint(0, 2, 100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

logistic_reg = LogisticRegression(random_state=42)
logistic_reg.fit(X_train, y_train)

logistic_predictions = logistic_reg.predict(X_test)

logistic_accuracy = accuracy_score(y_test, logistic_predictions)

naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

naive_bayes_predictions = naive_bayes.predict(X_test)

naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_predictions)

models = ['Regressão Logística', 'Naive Bayes']
accuracies = [logistic_accuracy, naive_bayes_accuracy]

plt.figure(figsize=(8, 6))
plt.bar(models, accuracies, color=['blue', 'green'])
plt.ylabel('Acurácia')
plt.title('Comparação de Acurácia entre Modelos')
plt.ylim(0, 1)

for i, v in enumerate(accuracies):
    plt.text(i, v + 0.02, str(round(v, 2)), ha='center', va='bottom')

plt.show()

"""Possui o objetivo de comparar o desempenho de dois modelos de aprendizado de máquina, **Regressão Logística** e **Naive Bayes** (GaussianNB). O código realiza essa comparação e apresenta as diferenças de desempenho por meio de um gráfico de barras, utilizando a acurácia como métrica de avaliação.

## **Conclusão**

Referente aos dados do arquivo (train.csv), com execução dos códigos para fornecer a introdução ao fluxo de trabalho típico de machine learning, desde a preparação dos dados até a avaliação de modelos.

Os dados podem servir como base para construir e testar modelos mais sofisticados em projetos de machine learning com conjuntos de dados reais.

# **2 -	Criar um sistema especialista ou Usar um algoritmo não supervisionado na base e falar da experiencia.**

### **Valores problemáticos da coluna (Age)**
"""

problematic_values = data['Age'][pd.to_numeric(data['Age'], errors='coerce').isna()].unique()
print("Problematic Values in 'Age' column:", problematic_values)

"""Identificar e imprimir os valores problemáticos na coluna 'Age' do conjunto de dados.

### **Tratando Valores Não Numéricos na Coluna (Age)**
"""

data = data[pd.to_numeric(data['Age'], errors='coerce').notna()]

"""Analisar e filtrar o conjunto de dados para manter apenas as linhas em que a coluna 'Age' contém valores numéricos válidos, descartando as linhas em que a conversão para numérico resultaria em NaN (Not-a-Number)."""

non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

for column in non_numeric_columns:
    problematic_values = data[column][pd.to_numeric(data[column], errors='coerce').isna()].unique()
    print(f"Problematic Values in '{column}' column:", problematic_values)

"""### **Remoção de Linhas com Valores Ausentes ou Não Numéricos**"""

data = data.dropna(subset=non_numeric_columns)

"""Possui o objetivo de exclui todas as entradas que possuem valores não numéricos ou ausentes em colunas não numéricas.

### **Analise de valores problemáticos em colunas não numéricas**
"""

relevant_columns = ['Age', 'Annual_Income']
non_numeric_columns = data[relevant_columns].select_dtypes(exclude=[np.number]).columns

for column in non_numeric_columns:
    problematic_values = data[column][pd.to_numeric(data[column], errors='coerce').isna()].unique()
    print(f"Problematic Values in '{column}' column:", problematic_values)

"""Identificar e imprimir os valores problemáticos nas colunas que não são do tipo numérico no conjunto de dados.

### **Padronização de Colunas selecionadas para Valores Numéricos Válidos**
"""

data[relevant_columns] = data[relevant_columns].apply(pd.to_numeric, errors='coerce').fillna(0)

"""Com o objetico de converter e padronizar as colunas selecionadas com conjunto de dados, transformando todas as entradas em números e preenchendo quaisquer valores não numéricos com 0.

Assim podendo garantir que as colunas relevantes obtenham somente valores numéricos válidos.

### **Limpeza e Análise de PCA de dados com visualização**
"""

data = data[pd.to_numeric(data['Age'], errors='coerce').notna()]
data = data[pd.to_numeric(data['Annual_Income'], errors='coerce').notna()]

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

X = data[['Age', 'Annual_Income']]
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA')
plt.show()

explained_variance = pca.explained_variance_ratio_
print("Explained Variance Ratio:")
print(explained_variance)

"""Possui o objetivo de realizar a limpeza dos dados, eliminando as linhas com valores não numéricos em colunas específicas, com uma análise de Componentes Principais (PCA) nas colunas para redução de dimensionalidade e finalmente obtendo a visualização dos resultados dessa redução.

Assim auxilia na compreensão de como as colunas 'Age' e 'Annual_Income' contribuem para a variação nos dados.

## **Conclusão**

Após a limpeza dos dados problemáticos, utilizamos a análise de componentes principais (PCA) para visualizar como as variáveis `Age' e 'Annual_Income' estão relacionadas em um espaço bidimensional. Ao medir a importância dos componentes principais, foi determinada a contribuição relativa de cada variável para a variabilidade dos dados.

A limpeza de dados e a análise de PCA são etapas valiosas na preparação dos dados para análise posterior, fornecendo informações sobre a estrutura e a importância das variáveis envolvidas. Essa abordagem o ajudará a tomar decisões sobre as análises subsequentes.
"""